{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           id                                               link  \\\n",
      "0  1698308935  https://twitter.com/realDonaldTrump/status/169...   \n",
      "1  1701461182  https://twitter.com/realDonaldTrump/status/170...   \n",
      "2  1737479987  https://twitter.com/realDonaldTrump/status/173...   \n",
      "3  1741160716  https://twitter.com/realDonaldTrump/status/174...   \n",
      "4  1773561338  https://twitter.com/realDonaldTrump/status/177...   \n",
      "\n",
      "                                             content                 date  \\\n",
      "0  Be sure to tune in and watch Donald Trump on L...  2009-05-04 13:54:25   \n",
      "1  Donald Trump will be appearing on The View tom...  2009-05-04 20:00:10   \n",
      "2  Donald Trump reads Top Ten Financial Tips on L...  2009-05-08 08:38:08   \n",
      "3  New Blog Post: Celebrity Apprentice Finale and...  2009-05-08 15:40:15   \n",
      "4  \"My persona will never be that of a wallflower...  2009-05-12 09:07:28   \n",
      "\n",
      "   retweets  favorites mentions hashtags  \n",
      "0       510        917      NaN      NaN  \n",
      "1        34        267      NaN      NaN  \n",
      "2        13         19      NaN      NaN  \n",
      "3        11         26      NaN      NaN  \n",
      "4      1375       1945      NaN      NaN  \n",
      "       id ticker                                              title category  \\\n",
      "0  221515    NIO  Why Shares of Chinese Electric Car Maker NIO A...     news   \n",
      "1  221516    NIO  NIO only consumer gainer  Workhorse Group amon...     news   \n",
      "2  221517    NIO  NIO leads consumer gainers  Beyond Meat and Ma...     news   \n",
      "3  221518    NIO                  NIO  NVAX among premarket gainers     news   \n",
      "4  221519    NIO                  PLUG  NIO among premarket gainers     news   \n",
      "\n",
      "                                             content release_date  \\\n",
      "0  What s happening\\nShares of Chinese electric c...   2020-01-15   \n",
      "1  Gainers  NIO  NYSE NIO   7  \\nLosers  MGP Ingr...   2020-01-18   \n",
      "2  Gainers  NIO  NYSE NIO   14   Village Farms In...   2020-01-15   \n",
      "3  Cemtrex  NASDAQ CETX   85  after FY results \\n...   2020-01-15   \n",
      "4  aTyr Pharma  NASDAQ LIFE   63  on Kyorin Pharm...   2020-01-06   \n",
      "\n",
      "          provider                                                url  \\\n",
      "0  The Motley Fool                             https://invst.ly/pigqi   \n",
      "1    Seeking Alpha                             https://invst.ly/pje9c   \n",
      "2    Seeking Alpha                             https://invst.ly/pifmv   \n",
      "3    Seeking Alpha                             https://invst.ly/picu8   \n",
      "4    Seeking Alpha  https://seekingalpha.com/news/3529772-plug-nio...   \n",
      "\n",
      "   article_id  \n",
      "0     2060327  \n",
      "1     2062196  \n",
      "2     2060249  \n",
      "3     2060039  \n",
      "4     2053096  \n",
      "   Unnamed: 0        date      timestamp  \\\n",
      "0           0  2012/10/01  1349064000000   \n",
      "1           1  2012/10/01  1349064000000   \n",
      "2           2  2012/10/01  1349064000000   \n",
      "3           3  2012/10/01  1349064000000   \n",
      "4           4  2012/10/01  1349064000000   \n",
      "\n",
      "                                               title  level2  level3  \n",
      "0  Catchings, January help Fever even series with...  sports    wnba  \n",
      "1  Kyle Busch rants on radio after his Toyota fal...  sports  nascar  \n",
      "2  Schwarzenegger says 'You can't run from your m...    life   books  \n",
      "3                                    Ryder Cup Day 3  sports    golf  \n",
      "4  Regular officials blow another big call agains...  gameon     NaN  \n",
      "         Date     Open    High      Low    Close  Adj Close    Volume\n",
      "0  2016-11-01  4802.75  4817.0  4718.75  4757.25    4757.25  307806.0\n",
      "1  2016-11-02  4754.50  4768.5  4708.25  4716.75    4716.75  300313.0\n",
      "2  2016-11-03  4700.00  4729.5  4668.25  4674.50    4674.50  280599.0\n",
      "3  2016-11-04  4671.25  4695.5  4651.25  4657.75    4657.75  264048.0\n",
      "4  2016-11-06      NaN     NaN      NaN      NaN        NaN       NaN\n",
      "         Date          Open          High           Low         Close  \\\n",
      "0  2016-11-01  18158.240234  18177.009766  17940.839844  18037.099609   \n",
      "1  2016-11-02  18017.720703  18044.150391  17931.890625  17959.640625   \n",
      "2  2016-11-03  17978.750000  18006.960938  17904.070313  17930.669922   \n",
      "3  2016-11-04  17928.349609  17986.759766  17883.560547  17888.279297   \n",
      "4  2016-11-07  17994.640625  18263.300781  17994.640625  18259.599609   \n",
      "\n",
      "      Adj Close     Volume  \n",
      "0  18037.099609  101280000  \n",
      "1  17959.640625   88610000  \n",
      "2  17930.669922   77860000  \n",
      "3  17888.279297   97760000  \n",
      "4  18259.599609   93450000  \n",
      "         Date     Open     High      Low    Close  Adj Close     Volume\n",
      "0  2016-11-01  2123.50  2129.50  2091.00  2103.75    2103.75  2257062.0\n",
      "1  2016-11-02  2102.25  2106.50  2087.25  2092.25    2092.25  2013595.0\n",
      "2  2016-11-03  2088.25  2098.75  2079.75  2083.50    2083.50  1795490.0\n",
      "3  2016-11-04  2084.00  2094.25  2078.75  2080.00    2080.00  2079129.0\n",
      "4  2016-11-06      NaN      NaN      NaN      NaN        NaN        NaN\n",
      "        Date                                              title\n",
      "0 2020-01-15  Why Shares of Chinese Electric Car Maker NIO A...\n",
      "1 2020-01-18  NIO only consumer gainer  Workhorse Group amon...\n",
      "4 2020-01-06  PLUG  NIO among premarket gainers. NIO up 6  o...\n",
      "5 2019-12-31  NIO leads consumer gainers  Origin Agritech on...\n",
      "6 2020-01-07  Beyond Meat tops consumer gainers  NIO and Eas...\n",
      "         date                                              title  level2\n",
      "0  2012/10/01  Catchings, January help Fever even series with...  sports\n",
      "1  2012/10/01  Kyle Busch rants on radio after his Toyota fal...  sports\n",
      "2  2012/10/01  Schwarzenegger says 'You can't run from your m...    life\n",
      "3  2012/10/01                                    Ryder Cup Day 3  sports\n",
      "4  2012/10/01  Regular officials blow another big call agains...  gameon\n",
      "             Date                                              title\n",
      "566274 2016-11-08  FTC sends $3.7M to victims of pyramid scheme. ...\n",
      "566747 2016-11-09  Trump website glitch fixed, but not before Int...\n",
      "567390 2016-11-10  Every bit of positivity counts. Here's how to ...\n",
      "567966 2016-11-11  Student gives 'deportation' notices to other k...\n",
      "568476 2016-11-12  Clintons are a 'talented family,' Trump tells ...\n",
      "            Date                                            content\n",
      "30887 2016-11-08  Today we are going to win the great state of M...\n",
      "30897 2016-11-09  Such a beautiful and important evening! The fo...\n",
      "30898 2016-11-10  Happy 241st birthday to the U.S. Marine Corps!...\n",
      "30901 2016-11-11  Love the fact that the small groups of protest...\n",
      "30904 2016-11-12  This will prove to be a great time in the live...\n",
      "         Date  Up_sp\n",
      "6  2016-11-07      1\n",
      "7  2016-11-08      1\n",
      "8  2016-11-09      1\n",
      "10 2016-11-13      0\n",
      "11 2016-11-14      1\n",
      "         Date  Close_sp\n",
      "6  2016-11-07   2135.50\n",
      "7  2016-11-08   2160.25\n",
      "8  2016-11-09   2167.25\n",
      "10 2016-11-13   2160.50\n",
      "11 2016-11-14   2179.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base model for each stock\n",
      "Dow:  0.5590851334180432\n",
      "sp:  0.5933926302414231\n",
      "nas:  0.5921219822109276\n",
      "Found 1193514 word vectors.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "path=\"C:\\\\Users\\\\marwe\\\\Desktop\\\\Trump_tweet_news_stock\"\n",
    "#read trump tweets\n",
    "df_tweets=pd.read_csv(path+\"\\\\realdonaldtrump.csv\")\n",
    "print(df_tweets.head())\n",
    "\n",
    "\n",
    "#stock news\n",
    "df_stock_news=pd.read_csv(\"C:\\\\Users\\\\marwe\\\\Desktop\\\\LaTeX JOTA\\\\us_equities_news_dataset.csv\")\n",
    "print(df_stock_news.head())\n",
    "\n",
    "#read the news dataset from usa today\n",
    "df_news=pd.read_csv(path+\"\\\\USA Today.csv\")\n",
    "print(df_news.head())\n",
    "\n",
    "#read nasdaq, s&p500 and dow\n",
    "\n",
    "df_nasdaq=pd.read_csv(path+\"\\\\NASDAQ_price.csv\")\n",
    "print(df_nasdaq.head())\n",
    "\n",
    "\n",
    "\n",
    "df_Dow=pd.read_csv(path+\"\\\\DowJones_price.csv\")\n",
    "print(df_Dow.head())\n",
    "\n",
    "df_sp=pd.read_csv(\"C:\\\\Users\\\\marwe\\\\Desktop\\\\Trump_tweet_news_stock\\\\SP_price.csv\")\n",
    "print(df_sp.head())\n",
    "\n",
    "\n",
    "\n",
    "#preparing the news dataset\n",
    "'''\n",
    "we will first rearrange the news then add columns for the stock market\n",
    "dates are between election and 2020-04-03 (one day after the news stop)\n",
    "\n",
    "'''\n",
    "\n",
    "df_stock_news=df_stock_news.rename(columns={\"release_date\":\"Date\"})\n",
    "df_stock_news['Date'] = pd.to_datetime(df_stock_news['Date']) \n",
    "df_stock_news['Date'] = pd.to_datetime(df_stock_news['Date']) \n",
    "mask = (df_stock_news['Date'] >= '2016-11-8' )\n",
    "df_stock_news=df_stock_news.loc[mask]\n",
    "col=['Date','title']\n",
    "df_stock_news=df_stock_news[col]\n",
    "\n",
    "#df_stock_news.content = np.where(df_stock_news.content.isnull(),str(df_stock_news.Date),str(df_stock_news.content))\n",
    "#toronto_df['Neighbourhood'] = toronto_df.groupby(['Postcode','Borough'])['Neighbourhood'].agg(lambda x: ','.join(x))\n",
    "\n",
    "df_stock_news['title']=df_stock_news['title'].fillna(\" \")\n",
    "df_stock_news['title'] = df_stock_news[['Date','title']].groupby(['Date'])['title'].transform(lambda x: '. '.join(x))\n",
    "df_stock_news=df_stock_news[['Date','title']].drop_duplicates()\n",
    "print(df_stock_news.head())\n",
    "\n",
    "#col_selection=[\"news\",\"money\",\"onpolitics\",\"theoval\"]\n",
    "col=['date','title','level2']\n",
    "df_news=df_news[col]\n",
    "print(df_news.head())\n",
    "#df_news=df_news.loc[df_news['level2'] != 'sports']\n",
    "#df_news=df_news.loc[df_news['level2'] != 'entertainment']\n",
    "#df_news=df_news.loc[df_news['level2'] != 'travel']\n",
    "#df_news=df_news.loc[df_news['level2'] != 'gameon']\n",
    "#df_news=df_news.loc[df_news['level2'] != 'music']\n",
    "df_news=df_news.loc[(df_news['level2'] == \"news\")|(df_news['level2'] == \"money\")|(df_news['level2'] == \"onpolitics\")|(df_news['level2'] == \"theoval\")]\n",
    "#pick between the dates needed\n",
    "df_news=df_news.rename(columns={\"date\":\"Date\"})\n",
    "df_news['Date'] = pd.to_datetime(df_news['Date']) \n",
    "mask = (df_news['Date'] >= '2016-11-8' )\n",
    "df_news=df_news.loc[mask]\n",
    "col=['Date','title']\n",
    "df_news=df_news[col]\n",
    "df_news['title'] = df_news[['Date','title']].groupby(['Date'])['title'].transform(lambda x: '. '.join(x))\n",
    "df_news=df_news[['Date','title']].drop_duplicates()\n",
    "print(df_news.head())\n",
    "\n",
    "def sub_one(x):\n",
    "\treturn x -timedelta(days=1)\n",
    "\n",
    "#make sure dow nasdaq and s&p are recorded between the 2 dates\n",
    "#dow\n",
    "df_Dow['Date'] = pd.to_datetime(df_Dow['Date']) \n",
    "mask = (df_Dow['Date'] >= '2016-11-8') &  (df_Dow['Date'] <= '2020-4-3')\n",
    "df_Dow=df_Dow.loc[mask]\n",
    "colo=['Date','Open','Close','Adj Close']\n",
    "df_Dow=df_Dow[colo]\n",
    "df_Dow['Adj Close']=np.where(df_Dow['Close']>=df_Dow['Open'],1,0)\n",
    "df_Dow=df_Dow.rename(columns={\"Adj Close\":\"Up_Dow\"})\n",
    "df_Dow=df_Dow.rename(columns={\"Close\":\"Close_Dow\"})\n",
    "df_Dow['Date']=df_Dow['Date'].apply(sub_one)\n",
    "\n",
    "#col_Dow=['Date','Up_Dow']\n",
    "#df_Dow=df_Dow.loc[col_Dow]\n",
    "\n",
    "\n",
    "#nasdaq\n",
    "df_nasdaq['Date'] = pd.to_datetime(df_nasdaq['Date']) \n",
    "mask = (df_nasdaq['Date'] >= '2016-11-8') &  (df_nasdaq['Date'] <= '2020-4-3')\n",
    "df_nasdaq=df_nasdaq.loc[mask]\n",
    "df_nasdaq=df_nasdaq.dropna()\n",
    "df_nasdaq=df_nasdaq[colo]\n",
    "df_nasdaq['Adj Close']=np.where(df_nasdaq['Close']>=df_nasdaq['Open'],1,0)\n",
    "df_nasdaq=df_nasdaq.rename(columns={\"Adj Close\":\"Up_nas\"})\n",
    "df_nasdaq=df_nasdaq.rename(columns={\"Close\":\"Close_nas\"})\n",
    "df_nasdaq['Date']=df_nasdaq['Date'].apply(sub_one)\n",
    "#col_nasdaq=['Date','Close_nas','Up_nas']\n",
    "#df_nasdaq=df_nasdaq.loc[col_nasdaq]\n",
    "\n",
    "#SP\n",
    "df_sp['Date'] = pd.to_datetime(df_sp['Date']) \n",
    "mask = (df_sp['Date'] >= '2016-11-8') &  (df_sp['Date'] <= '2020-4-3')\n",
    "df_sp=df_sp.loc[mask]\n",
    "df_sp=df_sp.dropna()\n",
    "df_sp=df_sp[colo]\n",
    "df_sp['Adj Close']=np.where(df_sp['Close']>=df_sp['Open'],1,0)\n",
    "df_sp=df_sp.rename(columns={\"Adj Close\":\"Up_sp\"})\n",
    "df_sp=df_sp.rename(columns={\"Close\":\"Close_sp\"})\n",
    "df_sp['Date']=df_sp['Date'].apply(sub_one)\n",
    "\n",
    "#col_sp=['Date','Close_sp','Up_sp']\n",
    "#df_sp=df_sp.loc[col_sp]\n",
    "\n",
    "#trying merge\n",
    "margo=pd.merge(df_news, df_Dow, how='outer', on='Date')\n",
    "margo=pd.merge(margo,df_nasdaq, how='outer', on='Date')\n",
    "df_news_pred=pd.merge(margo,df_sp, how='outer', on='Date')\n",
    "df_news_pred=pd.merge(df_news_pred,df_stock_news, how='outer', on='Date')\n",
    "\n",
    "\n",
    "\n",
    "col_margo=['Date','title_x','Close_Dow','Up_Dow','Close_nas','Up_nas','Close_sp','Up_sp','title_y']\n",
    "df_news_pred=df_news_pred[col_margo]\n",
    "df_news_pred=df_news_pred.dropna()\n",
    "df_news_pred['title_x'] = df_news_pred[['Date','title_x','title_y']].groupby(['Date'])['title_x'].transform(lambda x: '. '.join(x))\n",
    "col_margo=['Date','title_x','Close_Dow','Up_Dow','Close_nas','Up_nas','Close_sp','Up_sp']\n",
    "df_news_pred=df_news_pred[col_margo]\n",
    "\n",
    "'''\n",
    "we created df_news_pred, a dataset that has daily news headlines as input and next day stock market prediction\n",
    "we will use this data to predict the stock market using possibly RNN \n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "Now we do the same for trump tweets\n",
    "'''\n",
    "#initial preprocessing and selecting the dates\n",
    "df_tweets=df_tweets.rename(columns={\"date\":\"Date\"})\n",
    "df_tweets['Date'] = pd.to_datetime(df_tweets['Date']) \n",
    "df_tweets = df_tweets.assign(Date = lambda x: pd.to_datetime(x['Date'].dt.strftime('%Y-%m-%d')))\n",
    "mask = (df_tweets['Date'] >= '2016-11-8') &  (df_tweets['Date'] <= '2020-4-3')\n",
    "df_tweets=df_tweets.loc[mask]\n",
    "\n",
    "\n",
    "#merge tweets according to date\n",
    "df_tweets['content'] = df_tweets[['Date','content']].groupby(['Date'])['content'].transform(lambda x: '. '.join(x))\n",
    "df_tweets=df_tweets[['Date','content']].drop_duplicates()\n",
    "print(df_tweets.head())\n",
    "\n",
    "#construct the input/output table with next day prediction\n",
    "margo=pd.merge(df_tweets, df_Dow, how='outer', on='Date')\n",
    "margo=pd.merge(margo,df_nasdaq, how='outer', on='Date')\n",
    "df_tweet_pred=pd.merge(margo,df_sp, how='outer', on='Date')\n",
    "\n",
    "col_margo=['Date','content','Close_Dow','Up_Dow','Close_nas','Up_nas','Close_sp','Up_sp']\n",
    "df_tweet_pred=df_tweet_pred[col_margo]\n",
    "df_tweet_pred=df_tweet_pred.dropna()\n",
    "\n",
    "\n",
    "'''\n",
    "we created df_tweet_pred, a dataset that has daily trump tweets as input and next day stock market prediction\n",
    "we will use this data to predict the stock market using possibly RNN \n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "now we prepare the time series data, we want the data to look like the following\n",
    "5 past observations of stock market info as input and then todays market info as output\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# up an down table for sp\n",
    "\n",
    "df_sp_his=pd.DataFrame()\n",
    "df_sp_his['Date']=df_sp['Date']\n",
    "df_sp_his['Up_sp']=df_sp['Up_sp']\n",
    "print(df_sp_his.head())\n",
    "df_sp_his['t-1sp']=df_sp['Up_sp'].shift(1)\n",
    "df_sp_his['t-2sp']=df_sp['Up_sp'].shift(2)\n",
    "df_sp_his['t-3sp']=df_sp['Up_sp'].shift(3)\n",
    "df_sp_his['t-4sp']=df_sp['Up_sp'].shift(4)\n",
    "df_sp_his['t-5sp']=df_sp['Up_sp'].shift(5)\n",
    "df_sp_his['t-6sp']=df_sp['Up_sp'].shift(6)\n",
    "\n",
    "# up an down table for Dow\n",
    "df_dow_his=pd.DataFrame()\n",
    "df_dow_his['Date']=df_Dow['Date']\n",
    "df_dow_his['Up_Dow']=df_Dow['Up_Dow']\n",
    "df_dow_his['t-1Dow']=df_Dow['Up_Dow'].shift(1)\n",
    "df_dow_his['t-2Dow']=df_Dow['Up_Dow'].shift(2)\n",
    "df_dow_his['t-3Dow']=df_Dow['Up_Dow'].shift(3)\n",
    "df_dow_his['t-4Dow']=df_Dow['Up_Dow'].shift(4)\n",
    "df_dow_his['t-5Dow']=df_Dow['Up_Dow'].shift(5)\n",
    "df_dow_his['t-6Dow']=df_Dow['Up_Dow'].shift(6)\n",
    "\n",
    "# up an down table for nasdaq\n",
    "df_nas_his=pd.DataFrame()\n",
    "df_nas_his['Date']=df_nasdaq['Date']\n",
    "df_nas_his['Up_nas']=df_nasdaq['Up_nas']\n",
    "df_nas_his['t-1nas']=df_nasdaq['Up_nas'].shift(1)\n",
    "df_nas_his['t-2nas']=df_nasdaq['Up_nas'].shift(2)\n",
    "df_nas_his['t-3nas']=df_nasdaq['Up_nas'].shift(3)\n",
    "df_nas_his['t-4nas']=df_nasdaq['Up_nas'].shift(4)\n",
    "df_nas_his['t-5nas']=df_nasdaq['Up_nas'].shift(5)\n",
    "df_nas_his['t-6nas']=df_nasdaq['Up_nas'].shift(6)\n",
    "\n",
    "\n",
    "#same for the price\n",
    "\n",
    "# price table for sp\n",
    "df_spp_his=pd.DataFrame()\n",
    "df_spp_his['Date']=df_sp['Date']\n",
    "df_spp_his['Close_sp']=df_sp['Close_sp']\n",
    "print(df_spp_his.head())\n",
    "df_spp_his['t-1spp']=df_sp['Close_sp'].shift(1)\n",
    "df_spp_his['t-2spp']=df_sp['Close_sp'].shift(2)\n",
    "df_spp_his['t-3spp']=df_sp['Close_sp'].shift(3)\n",
    "df_spp_his['t-4spp']=df_sp['Close_sp'].shift(4)\n",
    "df_spp_his['t-5spp']=df_sp['Close_sp'].shift(5)\n",
    "df_spp_his['t-6spp']=df_sp['Close_sp'].shift(6)\n",
    "\n",
    "# price for Dow\n",
    "df_dowp_his=pd.DataFrame()\n",
    "df_dowp_his['Date']=df_Dow['Date']\n",
    "df_dowp_his['Close_Dow']=df_Dow['Close_Dow']\n",
    "df_dowp_his['t-1Dowp']=df_Dow['Close_Dow'].shift(1)\n",
    "df_dowp_his['t-2Dowp']=df_Dow['Close_Dow'].shift(2)\n",
    "df_dowp_his['t-3Dowp']=df_Dow['Close_Dow'].shift(3)\n",
    "df_dowp_his['t-4Dowp']=df_Dow['Close_Dow'].shift(4)\n",
    "df_dowp_his['t-5Dowp']=df_Dow['Close_Dow'].shift(5)\n",
    "df_dowp_his['t-6Dowp']=df_Dow['Close_Dow'].shift(6)\n",
    "\n",
    "# price for nasdaq\n",
    "df_nasp_his=pd.DataFrame()\n",
    "df_nasp_his['Date']=df_nasdaq['Date']\n",
    "df_nasp_his['Close_nas']=df_nasdaq['Close_nas']\n",
    "df_nasp_his['t-1nasp']=df_nasdaq['Close_nas'].shift(1)\n",
    "df_nasp_his['t-2nasp']=df_nasdaq['Close_nas'].shift(2)\n",
    "df_nasp_his['t-3nasp']=df_nasdaq['Close_nas'].shift(3)\n",
    "df_nasp_his['t-4nasp']=df_nasdaq['Close_nas'].shift(4)\n",
    "df_nasp_his['t-5nasp']=df_nasdaq['Close_nas'].shift(5)\n",
    "df_nasp_his['t-6nasp']=df_nasdaq['Close_nas'].shift(6)\n",
    "\n",
    "'''\n",
    "lets recap the input we got so far:\n",
    "df_news_pred: has daily news headlines and market predictions for sp/dow/nas\n",
    "df_tweet_pred: has daily trump tweets and then market predictions\n",
    "\n",
    "lastly we have time series data for nas/sp/dow for ups downs and prices (6 datasets in total)\n",
    "\n",
    "a total of 8 useful datasets, need to figure out a way to clean it further and construct a model\n",
    "'''\n",
    "\n",
    "'''\n",
    "lets try to merge some stuff\n",
    "'''\n",
    "\n",
    "merge_twt_news=pd.merge(df_tweet_pred,df_news_pred, how='outer', on=['Date','Close_Dow','Up_Dow','Close_nas','Up_nas','Close_sp','Up_sp'])\n",
    "#merge_twt_news=pd.merge(merge_twt_news,df_sp_his, how='outer', on=['Date','Up_sp'])\n",
    "#merge_twt_news=pd.merge(merge_twt_news,df_nas_his, how='outer', on=['Date','Up_nas'])\n",
    "merge_twt_news=pd.merge(merge_twt_news,df_sp_his, how='outer', on=['Date','Up_sp'])\n",
    "merge_twt_news=pd.merge(merge_twt_news,df_dow_his, how='outer', on=['Date','Up_Dow'])\n",
    "merge_twt_news=pd.merge(merge_twt_news,df_nas_his, how='outer', on=['Date','Up_nas'])\n",
    "\n",
    "merge_twt_news=merge_twt_news.dropna()\n",
    "\n",
    "\n",
    "'''\n",
    "lets preprocess the text columns first, we will preprocess tweets and news seperately\n",
    "'''\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras import preprocessing\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from keras import layers\n",
    "from keras import Input\n",
    "from keras.models import Model\n",
    "from keras import regularizers\n",
    "\n",
    "twt_words=20000\n",
    "maxlen_twt=500\n",
    "\n",
    "tok_tweet = Tokenizer(num_words=twt_words)\n",
    "tok_tweet.fit_on_texts(merge_twt_news['content'])\n",
    "merge_twt_news['content']=tok_tweet.texts_to_sequences(merge_twt_news['content'])\n",
    "x_tweet = preprocessing.sequence.pad_sequences(merge_twt_news['content'], maxlen=500)\n",
    "\n",
    "\n",
    "#do the same fro the news\n",
    "\n",
    "news_words=30000\n",
    "maxlen_news=1500\n",
    "\n",
    "tok_news = Tokenizer(num_words=news_words)\n",
    "tok_news.fit_on_texts(merge_twt_news['title_x'])\n",
    "merge_twt_news['title_x']=tok_news.texts_to_sequences(merge_twt_news['title_x'])\n",
    "x_news = preprocessing.sequence.pad_sequences(merge_twt_news['title_x'], maxlen=maxlen_news)\n",
    "merge_twt_news=merge_twt_news.drop(['content', 'title_x'], axis=1)\n",
    "#col_dow=['t-6Dow','t-5Dow','t-4Dow','t-3Dow','t-2Dow','t-1Dow','t-6sp','t-5sp','t-4sp','t-3sp','t-2sp','t-1sp','t-6nas','t-5nas','t-4nas','t-3nas','t-2nas','t-1nas']\n",
    "\n",
    "\n",
    "col_sp=['t-6sp','t-5sp','t-4sp','t-3sp','t-2sp','t-1sp']\n",
    "col_nas=['t-6nas','t-5nas','t-4nas','t-3nas','t-2nas','t-1nas']\n",
    "col_dow=['t-6Dow','t-5Dow','t-4Dow','t-3Dow','t-2Dow','t-1Dow']\n",
    "#y=merge_twt_news['Up_Dow']\n",
    "\n",
    "\"\"\"\n",
    "Constructing our X dataset for each stock\n",
    "\n",
    "\"\"\"\n",
    "#Dow\n",
    "X_dowp=merge_twt_news[col_dow]\n",
    "X_dowp=np.hstack((x_tweet,x_news,X_dowp))\n",
    "\n",
    "#nas\n",
    "X_nasp=merge_twt_news[col_nas]\n",
    "X_nasp=np.hstack((x_tweet,x_news,X_nasp))\n",
    "\n",
    "#sp\n",
    "X_spp=merge_twt_news[col_sp]\n",
    "X_spp=np.hstack((x_tweet,x_news,X_spp))\n",
    "\n",
    "#X_dowp_train,X_dowp_test,y_dowp_train,y_dowp_test= train_test_split(X_dowp,merge_twt_news[['Up_Dow','Up_sp','Up_nas']],test_size=0.25,random_state=0) #['Up_Dow'] for class\n",
    "\n",
    "X_dowp_train,X_dowp_test,y_dowp_train,y_dowp_test= train_test_split(X_dowp,merge_twt_news[['Up_Dow']],test_size=0.01,random_state=42)\n",
    "X_nasp_train,X_nasp_test,y_nasp_train,y_nasp_test= train_test_split(X_nasp,merge_twt_news[['Up_nas']],test_size=0.01,random_state=42)\n",
    "X_spp_train,X_spp_test,y_spp_train,y_spp_test= train_test_split(X_spp,merge_twt_news[['Up_sp']],test_size=0.01,random_state=42)\n",
    "#X_dowp_test,X_dowp_val,y_dowp_test,y_dowp_val= train_test_split(X_dowp_test,y_dowp_test,test_size=0.5,random_state=0,stratify=y_dowp_test)\n",
    "\n",
    "\n",
    "\n",
    "word_index_twitter = tok_tweet.word_index\n",
    "word_index_news = tok_news.word_index\n",
    "\n",
    "\n",
    "\n",
    "#skf=StratifiedKFold(n_splits=10,random_state=0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "LETS START WORKING ON THE MODEL\n",
    "'''\n",
    "\n",
    "'''\n",
    "any model that we work on needs to beat the base model: we assume given the current economy\n",
    "that the stock market will be rising (dow in this case) so prediction will always be 1\n",
    "'''\n",
    "y_base=np.ones(len( X_dowp_train),)\n",
    "print(\"Base model for each stock\")\n",
    "print(\"Dow: \",np.mean(y_base==y_dowp_train['Up_Dow']))\n",
    "print(\"sp: \",np.mean(y_base==y_spp_train['Up_sp']))\n",
    "print(\"nas: \",np.mean(y_base==y_nasp_train['Up_nas'])) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Start updating here please\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "#2nd model no twitter but rnn\n",
    "import matplotlib.pyplot as plt\n",
    "acc = history2.history['accuracy']\n",
    "val_acc = history2.history['val_accuracy']\n",
    "loss = history2.history['loss']\n",
    "val_loss = history2.history['val_loss']\n",
    "epochs = range(1, len(acc) + 1)\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy without twitter but rnn')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "#plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "#plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "#plt.title('Training and validation loss no twitt but rnn')\n",
    "#plt.legend()\n",
    "#plt.show()\n",
    "\n",
    "\n",
    "#3rd model rnn and twitter\n",
    "import matplotlib.pyplot as plt\n",
    "acc = history3.history['accuracy']\n",
    "val_acc = history3.history['val_accuracy']\n",
    "loss = history3.history['loss']\n",
    "val_loss = history3.history['val_loss']\n",
    "epochs = range(1, len(acc) + 1)\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy with twitter and rnn')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "#plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "#plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "#plt.title('Training and validation loss with twitt and rnn')\n",
    "#plt.legend()\n",
    "#plt.show()\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "'''\n",
    "now we need to start the regularization process as well as adding things to help\n",
    "solve the scarcity of data like pretrained embeddings \n",
    "also be careful we have a very big overfitting problem\n",
    "'''\n",
    "\n",
    "'''\n",
    "first we will try to handle lack of data by introducting a pre trained embedding vectors\n",
    "we will use the twitter GLOVE pretrained word vector\n",
    "lets prepare everything\n",
    "'''\n",
    "\n",
    "#unzip the twitter glove and setup the index\n",
    "import os\n",
    "\n",
    "glove_dir = 'C:\\\\Users\\\\marwe\\\\Desktop\\\\Glove'\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join(glove_dir, 'glove.twitter.27B.100d.txt'),encoding=\"utf8\")\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "\n",
    "#setup the vocab vector for the twitter dataset\n",
    "embedding_dim = 100\n",
    "embedding_matrix_tweet = np.zeros((twt_words, embedding_dim))\n",
    "for word, i in word_index_twitter.items():\n",
    "    if i < twt_words:\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix_tweet[i] = embedding_vector\n",
    "\n",
    "\n",
    "#setup the vocab vector for the news dataset\n",
    "embedding_dim = 100\n",
    "embedding_matrix_news = np.zeros((news_words, embedding_dim))\n",
    "for word, i in word_index_news.items():\n",
    "    if i < news_words:\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix_news[i] = embedding_vector\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all models constructed!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "construct all models\n",
    "\"\"\"\n",
    "\n",
    "def model_0():\n",
    "    input_news = Input(shape=(None,))\n",
    "    input_hist=Input(shape=(6,1))\n",
    "    \n",
    "    \n",
    "    \n",
    "    # the second branch opreates on the second input\n",
    "    y = layers.Embedding(input_dim=news_words+1,output_dim=64,input_length=1500,name='c')(input_news)\n",
    "    y=layers.Flatten()(y)\n",
    "    y = Model(inputs=input_news, outputs=y)\n",
    "    \n",
    "    # the 3rd branch opreates on the 3rd input\n",
    "    \n",
    "    z = layers.LSTM(3,kernel_regularizer=regularizers.l1_l2(l1=1e-3, l2=1e-3),\n",
    "        bias_regularizer=regularizers.l2(1e-3),\n",
    "        activity_regularizer=regularizers.l2(1e-3),name='f',return_sequences=True)(input_hist)\n",
    "    z=layers.Flatten()(z)\n",
    "    z = Model(inputs=input_hist, outputs=z)\n",
    "    \n",
    "    \n",
    "    # combine the output of the two branches\n",
    "    combined1 = layers.concatenate([y.output,z.output],axis=-1,name='g')\n",
    "    \n",
    "    # apply a FC layer and then a regression prediction on the\n",
    "    # combined outputs\n",
    "    m = layers.Dense(8,kernel_regularizer=regularizers.l1_l2(l1=1e-3, l2=1e-3),\n",
    "        bias_regularizer=regularizers.l2(1e-3),\n",
    "        activity_regularizer=regularizers.l2(1e-3), activation=\"relu\",name='h')(combined1)\n",
    "    \n",
    "    #dow = layers.Dense(1, activation=\"sigmoid\",name='i')(m)\n",
    "    #sp = layers.Dense(1, activation=\"sigmoid\",name='j')(m)\n",
    "    nas= layers.Dense(1, activation=\"sigmoid\",name='k')(m)\n",
    "    # our model will accept the inputs of the two branches and\n",
    "    # then output a single value\n",
    "    #model = Model(inputs=[ y.input,z.input], outputs=[dow,sp,nas])\n",
    "    model = Model(inputs=[ y.input,z.input], outputs=nas)\n",
    "    #model.summary()\n",
    "    model.compile(loss='binary_crossentropy', optimizer='Adam', metrics=['accuracy'])\n",
    "    #model.compile(loss='mse', optimizer='rmsprop', metrics=['mse'])\n",
    "    return model\n",
    "\n",
    "\n",
    "'''\n",
    "lets introduce RNN for the news to see if we get better results \n",
    "\n",
    "'''\n",
    "def model_1():\n",
    "    #input_tweet = Input(shape=(None,))\n",
    "    input_news = Input(shape=(None,))\n",
    "    input_hist=Input(shape=(6,1))\n",
    "    \n",
    "    \n",
    "    # the first branch operates on the first input\n",
    "    \n",
    "    \n",
    "    # the second branch opreates on the second input\n",
    "    y = layers.Embedding(input_dim=news_words,output_dim=64,input_length=1500,name='c')(input_news)\n",
    "    y= layers.LSTM(32,return_sequences=False,name='e')(y)\n",
    "    y = Model(inputs=input_news, outputs=y)\n",
    "    \n",
    "    # the 3rd branch opreates on the 3rd input\n",
    "    \n",
    "    z = layers.LSTM(3,kernel_regularizer=regularizers.l1_l2(l1=1e-3, l2=1e-3),\n",
    "        bias_regularizer=regularizers.l2(1e-3),\n",
    "        activity_regularizer=regularizers.l2(1e-3),name='f')(input_hist)\n",
    "    z = Model(inputs=input_hist, outputs=z)\n",
    "    \n",
    "    \n",
    "    # combine the output of the two branches\n",
    "    combined = layers.concatenate([y.output,z.output],axis=-1,name='g')\n",
    "    \n",
    "    # apply a FC layer and then a regression prediction on the\n",
    "    # combined outputs\n",
    "    m = layers.Dense(8,kernel_regularizer=regularizers.l1_l2(l1=1e-3, l2=1e-3),\n",
    "        bias_regularizer=regularizers.l2(1e-3),\n",
    "        activity_regularizer=regularizers.l2(1e-3), activation=\"relu\",name='h')(combined)\n",
    "    m = layers.Dense(1, activation=\"sigmoid\",name='i')(m)\n",
    "    # our model will accept the inputs of the two branches and\n",
    "    # then output a single value\n",
    "    model = Model(inputs=[input_news,input_hist], outputs=m)\n",
    "    #model.summary()\n",
    "    model.compile(loss='binary_crossentropy', optimizer='Adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "lets intorduce the twitter feed\n",
    "'''\n",
    "\n",
    "# define 3 sets of inputs\n",
    "def model_2():\n",
    "    input_tweet = Input(shape=(None,))\n",
    "    input_news = Input(shape=(None,))\n",
    "    input_hist=Input(shape=(6,1))\n",
    "    \n",
    "    \n",
    "    # the first branch operates on the first input\n",
    "    \n",
    "    x = layers.Embedding(input_dim=twt_words,output_dim=64,input_length=500,name='a')(input_tweet)\n",
    "    x = layers.LSTM(32,kernel_regularizer=regularizers.l1_l2(l1=1e-3, l2=1e-3),\n",
    "        bias_regularizer=regularizers.l2(1e-3),\n",
    "        activity_regularizer=regularizers.l2(1e-3), return_sequences=False,name='b')(x)\n",
    "    x = Model(inputs=input_tweet, outputs=x)\n",
    "    # the second branch opreates on the second input\n",
    "    y = layers.Embedding(input_dim=news_words,output_dim=64,input_length=1500,name='c')(input_news)\n",
    "    y= layers.LSTM(32,kernel_regularizer=regularizers.l1_l2(l1=1e-3, l2=1e-3),\n",
    "        bias_regularizer=regularizers.l2(1e-3),\n",
    "        activity_regularizer=regularizers.l2(1e-3),return_sequences=False,name='e')(y)\n",
    "    y = Model(inputs=input_news, outputs=y)\n",
    "    \n",
    "    # the 3rd branch opreates on the 3rd input\n",
    "    \n",
    "    z = layers.LSTM(3,kernel_regularizer=regularizers.l1_l2(l1=1e-3, l2=1e-3),\n",
    "        bias_regularizer=regularizers.l2(1e-3),\n",
    "        activity_regularizer=regularizers.l2(1e-3),name='f')(input_hist)\n",
    "    z = Model(inputs=input_hist, outputs=z)\n",
    "    \n",
    "    \n",
    "    # combine the output of the two branches\n",
    "    combined = layers.concatenate([x.output,y.output,z.output],axis=-1,name='g')\n",
    "    \n",
    "    # apply a FC layer and then a regression prediction on the\n",
    "    # combined outputs\n",
    "    m = layers.Dense(8,kernel_regularizer=regularizers.l1_l2(l1=1e-3, l2=1e-3),\n",
    "        bias_regularizer=regularizers.l2(1e-3),\n",
    "        activity_regularizer=regularizers.l2(1e-3), activation=\"relu\",name='h')(combined)\n",
    "    m = layers.Dense(1, activation=\"sigmoid\",name='i')(m)\n",
    "    # our model will accept the inputs of the two branches and\n",
    "    # then output a single value\n",
    "    model = Model(inputs=[input_tweet, input_news,input_hist], outputs=m)\n",
    "    #model.summary()\n",
    "    model.compile(loss='binary_crossentropy', optimizer='Adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "'''\n",
    "after loading the glove embedding vectorizer, lets try our previous model and see what we obtain\n",
    "'''\n",
    "# define 3 sets of inputs\n",
    "def model_3():\n",
    "    input_tweet = Input(shape=(None,))\n",
    "    input_news = Input(shape=(None,))\n",
    "    input_hist=Input(shape=(6,1))\n",
    "    \n",
    "    \n",
    "    # the first branch operates on the first input\n",
    "    \n",
    "    x = layers.Embedding(input_dim=twt_words,output_dim=embedding_dim,weights=[embedding_matrix_tweet],trainable=False,input_length=500,name='a')(input_tweet)\n",
    "    x = layers.LSTM(32,return_sequences=False,name='b')(x)\n",
    "    x = Model(inputs=input_tweet, outputs=x)\n",
    "    # the second branch opreates on the second input\n",
    "    y = layers.Embedding(input_dim=news_words,output_dim=embedding_dim,weights=[embedding_matrix_news],trainable=False,input_length=1500,name='c')(input_news)\n",
    "    y= layers.LSTM(32,return_sequences=False,name='e')(y)\n",
    "    y = Model(inputs=input_news, outputs=y)\n",
    "    \n",
    "    # the 3rd branch opreates on the 3rd input\n",
    "    \n",
    "    z = layers.LSTM(6,name='f')(input_hist) #used to be 3\n",
    "    z = Model(inputs=input_hist, outputs=z)\n",
    "    \n",
    "    \n",
    "    # combine the output of the two branches\n",
    "    combined = layers.concatenate([x.output,y.output,z.output],axis=-1,name='g')\n",
    "    \n",
    "    # apply a FC layer and then a regression prediction on the\n",
    "    # combined outputs\n",
    "    m = layers.Dense(16, activation=\"relu\",name='h')(combined)#used to be 8\n",
    "    m = layers.Dense(1, activation=\"sigmoid\",name='i')(m)\n",
    "    # our model will accept the inputs of the two branches and\n",
    "    # then output a single value\n",
    "    model = Model(inputs=[input_tweet, input_news,input_hist], outputs=m)\n",
    "    #model.summary()\n",
    "    model.compile(loss='binary_crossentropy', optimizer='Adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "lets try some regulirazation techniques\n",
    "'''\n",
    "\n",
    "def model_4():\n",
    "# define 3 sets of inputs\n",
    "    input_tweet = Input(shape=(None,))\n",
    "    input_news = Input(shape=(None,))\n",
    "    input_hist=Input(shape=(6,1))\n",
    "    \n",
    "    \n",
    "    # the first branch operates on the first input\n",
    "    \n",
    "    x = layers.Embedding(input_dim=twt_words,output_dim=embedding_dim,weights=[embedding_matrix_tweet],trainable=False,input_length=500,name='a')(input_tweet)\n",
    "    #x = layers.Bidirectional(layers.LSTM(32,return_sequences=False,recurrent_dropout=0.33,dropout=0.33,name='b'))(x)\n",
    "    x=(layers.LSTM(100,return_sequences=False,recurrent_dropout=0.33,dropout=0.33,name='b'))(x)\n",
    "    #x=(layers.LSTM(40,return_sequences=False,recurrent_dropout=0.3,dropout=0.3,name='t'))(x)\n",
    "    x = Model(inputs=input_tweet, outputs=x)\n",
    "    # the second branch opreates on the second input\n",
    "    y = layers.Embedding(input_dim=news_words,output_dim=embedding_dim,weights=[embedding_matrix_news],trainable=False,input_length=1500,name='c')(input_news)\n",
    "    #y= layers.Bidirectional(layers.LSTM(32,return_sequences=False,recurrent_dropout=0.2,dropout=0.2,name='e'))(y)\n",
    "    y=(layers.LSTM(80,return_sequences=False,recurrent_dropout=0.33,dropout=0.33,name='e'))(y)\n",
    "    #y=(layers.LSTM(40,return_sequences=False,recurrent_dropout=0.2,dropout=0.2,name='ae'))(y)\n",
    "    y = Model(inputs=input_news, outputs=y)\n",
    "    \n",
    "    # the 3rd branch opreates on the 3rd input\n",
    "    \n",
    "    #z = layers.Bidirectional(layers.LSTM(6,name='f',recurrent_dropout=0.1,dropout=0.1))(input_hist)\n",
    "    z=(layers.LSTM(6,name='f'))(input_hist)#used to be 3\n",
    "    z = Model(inputs=input_hist, outputs=z)\n",
    "    \n",
    "    \n",
    "    # combine the output of the two branches\n",
    "    combined = layers.concatenate([x.output,y.output,z.output],axis=-1,name='g')\n",
    "    \n",
    "    # apply a FC layer and then a regression prediction on the\n",
    "    # combined outputs\n",
    "    m = layers.Dense(10, activation=\"tanh\",name='h')(combined)\n",
    "    #m=layers.Dropout(0.1)(m)#used to be 8\n",
    "    m = layers.Dense(1, activation=\"sigmoid\",name='i')(m)\n",
    "    model = Model(inputs=[input_tweet, input_news,input_hist], outputs=m)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='Adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "print(\"all models constructed!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nas training\n",
      "Train on 661 samples, validate on 126 samples\n",
      "Epoch 1/25\n",
      "661/661 [==============================] - 85s 128ms/step - loss: 0.6850 - accuracy: 0.5749 - val_loss: 0.6737 - val_accuracy: 0.5952\n",
      "Epoch 2/25\n",
      "661/661 [==============================] - 78s 117ms/step - loss: 0.6647 - accuracy: 0.6097 - val_loss: 0.6766 - val_accuracy: 0.5714\n",
      "Epoch 3/25\n",
      "661/661 [==============================] - 75s 114ms/step - loss: 0.6436 - accuracy: 0.6263 - val_loss: 0.6883 - val_accuracy: 0.5714\n",
      "Epoch 4/25\n",
      "661/661 [==============================] - 76s 114ms/step - loss: 0.6264 - accuracy: 0.6460 - val_loss: 0.7132 - val_accuracy: 0.5714\n",
      "Epoch 5/25\n",
      "661/661 [==============================] - 78s 118ms/step - loss: 0.6047 - accuracy: 0.6793 - val_loss: 0.7995 - val_accuracy: 0.4365\n",
      "Epoch 6/25\n",
      "661/661 [==============================] - 76s 115ms/step - loss: 0.6099 - accuracy: 0.6641 - val_loss: 0.7323 - val_accuracy: 0.5714\n",
      "Epoch 7/25\n",
      "661/661 [==============================] - 77s 116ms/step - loss: 0.5816 - accuracy: 0.6914 - val_loss: 0.7788 - val_accuracy: 0.4603\n",
      "Epoch 8/25\n",
      "661/661 [==============================] - 75s 113ms/step - loss: 0.5717 - accuracy: 0.7171 - val_loss: 0.7588 - val_accuracy: 0.5476\n",
      "Epoch 9/25\n",
      "661/661 [==============================] - 76s 115ms/step - loss: 0.5550 - accuracy: 0.7156 - val_loss: 0.7791 - val_accuracy: 0.5794\n",
      "Epoch 10/25\n",
      "661/661 [==============================] - 79s 119ms/step - loss: 0.5177 - accuracy: 0.7625 - val_loss: 0.7969 - val_accuracy: 0.5635\n",
      "Epoch 11/25\n",
      "661/661 [==============================] - 85s 128ms/step - loss: 0.4944 - accuracy: 0.7655 - val_loss: 0.8077 - val_accuracy: 0.5397\n",
      "Epoch 12/25\n",
      "661/661 [==============================] - 79s 119ms/step - loss: 0.4920 - accuracy: 0.7716 - val_loss: 0.8321 - val_accuracy: 0.5794\n",
      "Epoch 13/25\n",
      "661/661 [==============================] - 77s 116ms/step - loss: 0.4827 - accuracy: 0.7655 - val_loss: 0.8592 - val_accuracy: 0.5873\n",
      "Epoch 14/25\n",
      "661/661 [==============================] - 72s 108ms/step - loss: 0.4336 - accuracy: 0.7837 - val_loss: 0.8861 - val_accuracy: 0.5794\n",
      "Epoch 15/25\n",
      "661/661 [==============================] - 55s 83ms/step - loss: 0.4019 - accuracy: 0.8154 - val_loss: 0.9043 - val_accuracy: 0.5079\n",
      "Epoch 16/25\n",
      "661/661 [==============================] - 55s 83ms/step - loss: 0.4359 - accuracy: 0.7837 - val_loss: 0.9171 - val_accuracy: 0.5238\n",
      "Epoch 17/25\n",
      "661/661 [==============================] - 53s 80ms/step - loss: 0.3613 - accuracy: 0.8563 - val_loss: 1.0429 - val_accuracy: 0.4921\n",
      "Epoch 18/25\n",
      "661/661 [==============================] - 57s 87ms/step - loss: 0.3764 - accuracy: 0.8321 - val_loss: 0.9877 - val_accuracy: 0.5556\n",
      "Epoch 19/25\n",
      "560/661 [========================>.....] - ETA: 7s - loss: 0.3524 - accuracy: 0.8518 "
     ]
    }
   ],
   "source": [
    "from keras import backend as K \n",
    "\n",
    "# Do some code, e.g. train and save model\n",
    "\n",
    "\n",
    "print(\"Nas training\")\n",
    "\n",
    "#history_nas=model_0().fit([X_nasp_train[:,500:2000],np.reshape(X_nasp_train[:,2000:],(len(X_nasp_train),6,1))], y_nasp_train['Up_nas'], epochs=25, validation_split =0.16,verbose=1)\n",
    "#history_nas_1=model_1().fit([X_nasp_train[:,500:2000],np.reshape(X_nasp_train[:,2000:],(len(X_nasp_train),6,1))], y_nasp_train['Up_nas'], epochs=25, validation_split =0.16,verbose=1)\n",
    "#history_nas_2=model_2().fit([X_nasp_train[:,:500],X_nasp_train[:,500:2000],np.reshape(X_nasp_train[:,2000:],(len( X_dowp_train),6,1))], y_nasp_train, epochs=25, batch_size=28, validation_split=0.16,verbose=1)\n",
    "#history_nas_3=model_3().fit([X_nasp_train[:,:500],X_nasp_train[:,500:2000],np.reshape(X_nasp_train[:,2000:],(len( X_dowp_train),6,1))], y_nasp_train, epochs=25, batch_size=28, validation_split=0.16,verbose=1)\n",
    "history_nas_4=model_4().fit([X_nasp_train[:,:500],X_nasp_train[:,500:2000],np.reshape(X_nasp_train[:,2000:],(len( X_dowp_train),6,1))], y_nasp_train, epochs=25, batch_size=28, validation_split=0.16,verbose=1)\n",
    "\n",
    "\n",
    "#nasdaq comparison\n",
    "#acc_nas = history_nas.history['accuracy']\n",
    "#val_acc_nas = history_nas.history['val_accuracy']\n",
    "#acc_nas_1 = history_nas_1.history['accuracy']\n",
    "#val_acc_nas_1 = history_nas_1.history['val_accuracy']\n",
    "#acc_nas_2 = history_nas_2.history['accuracy']\n",
    "#val_acc_nas_2 = history_nas_2.history['val_accuracy']\n",
    "#acc_nas_3 = history_nas_3.history['accuracy']\n",
    "#val_acc_nas_3 = history_nas_3.history['val_accuracy']\n",
    "acc_nas_4 = history_nas_4.history['accuracy']\n",
    "val_acc_nas_4 = history_nas_4.history['val_accuracy']\n",
    "\n",
    "epochs = range(1, len(acc_nas_4) + 1)\n",
    "\n",
    "#plt.plot(epochs, val_acc_nas, label='Val nas lvl0')\n",
    "#plt.plot(epochs, val_acc_nas_1, label='Val nas lvl1')\n",
    "#plt.plot(epochs, val_acc_nas_2, label='Val nas lvl2')\n",
    "#plt.plot(epochs, val_acc_nas_3, label='Val nas lvl3')\n",
    "plt.plot(epochs, val_acc_nas_4, label='Val nas lvl4')\n",
    "plt.title('validation comparison for nas')\n",
    "plt.legend()\n",
    "#plt.figure()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
